# -*- coding: utf-8 -*-
"""Copy of GradientDescent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T0SbWJuAFc2NJH6Ow7EqddV3-8ic12og
"""

!pip install autograd

# Commented out IPython magic to ensure Python compatibility.
#imports
import autograd.numpy as np
from autograd import grad
import matplotlib.pyplot as plt
import math
from IPython import display
# %matplotlib inline

def test(l):
  l=np.array(l)
  l=l.astype(np.float64)
  return l[0]**2 + l[1]**3

f = grad(test)
res = f(np.array([2,2]).astype(np.float64))
print(res)

def y_func(x,y):
  return x**2 + y**3

y_func_dx = grad(y_func,0)
y_func_dy = grad(y_func,1)

print(y_func_dx(2.0,2.0))
print(y_func_dy(2.0,2.0))

current_x_pos = 2.0
delta = 1e-8
numerical_grad = (y_func(current_x_pos + delta) - y_func(current_x_pos)) / delta
print("numerical: ",numerical_grad)
print("actual: ", y_derivative(current_x_pos))
print("autograd:", y_derivative_auto(current_x_pos))

x = np.arange(-100, 100, 0.1)
y = y_func(x)

current_x_pos = 80.0

current_pos = (current_x_pos, y_func(current_x_pos))

plt.plot(x, y)
plt.scatter(current_pos[0], current_pos[1], color="red")

lr = 0.01
current_pos = (current_x_pos, y_func(current_x_pos))
for _ in range(100):
  new_x = current_pos[0] - lr * y_derivative_auto(current_pos[0])
  new_y = y_func(new_x)
  current_pos = (new_x, new_y)
  plt.plot(x, y)
  plt.scatter(current_pos[0], current_pos[1], color="red")
  display.display(plt.gcf())
  plt.clf()
  display.clear_output(wait=True)
plt.plot(x, y)
plt.scatter(current_pos[0], current_pos[1], color="red")
print(current_pos)

a = [1,2,3]
b = [4,5,6]
for i,j in zip(a,b):
  print(i,j)

def simpleGeneratorFun():
  i = 1
  while True:
    yield i
    i=i+1

# x is a generator object
x = simpleGeneratorFun()

print(next(x))

def test():
  return 1,2

print(test())

a = [1,2,3]
b = [4,5,6]



for i,j in zip(a,b):
  print(i,j)

g = fun()

for i in g:
  print(i)

a,*b = [1,2,3]

print(a,b)